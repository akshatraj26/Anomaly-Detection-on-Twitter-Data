{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8841,"sourceType":"datasetVersion","datasetId":4133}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-21T11:02:57.298801Z","iopub.execute_input":"2024-06-21T11:02:57.299263Z","iopub.status.idle":"2024-06-21T11:02:57.726488Z","shell.execute_reply.started":"2024-06-21T11:02:57.299227Z","shell.execute_reply":"2024-06-21T11:02:57.725211Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/customer-support-on-twitter/sample.csv\n/kaggle/input/customer-support-on-twitter/twcs/twcs.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Load dataset\nfile_path = '/kaggle/input/customer-support-on-twitter/twcs/twcs.csv'\ndf = pd.read_csv(file_path)\n\n# Basic text cleaning function\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+', '', text)  # remove URLs\n    text = re.sub(r'\\W', ' ', text)  # remove special characters\n    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n    return text\n\ndf['cleaned_text'] = df['text'].apply(clean_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T11:03:42.692278Z","iopub.execute_input":"2024-06-21T11:03:42.692776Z","iopub.status.idle":"2024-06-21T11:04:55.196556Z","shell.execute_reply.started":"2024-06-21T11:03:42.692745Z","shell.execute_reply":"2024-06-21T11:04:55.195428Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Feature Extraction using BERT","metadata":{}},{"cell_type":"code","source":"!pip install transformers\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Function to get BERT embeddings\ndef get_bert_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n    return embeddings\n\n# Apply function to the dataset\ndf['embeddings'] = df['cleaned_text'].apply(get_bert_embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T11:05:11.381708Z","iopub.execute_input":"2024-06-21T11:05:11.382078Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2370c77710324d8097fd7682567a2792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7d0d92614a4487d966c76ddb1d2ff16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ab8b3198c4841ccb6d1a6abb85ee215"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d34b83215c41678fa39c141aa0fd4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e67bae3f5845518c0372d6152819b0"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Step 3: Anomaly Detection","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\n# Convert embeddings to a format suitable for model training\nembeddings = list(df['embeddings'])\nX = np.array(embeddings)\n\n# Train Isolation Forest model\nmodel = IsolationForest(contamination=0.05)\nmodel.fit(X)\n\n# Predict anomalies\ndf['anomaly'] = model.predict(X)\ndf['anomaly'] = df['anomaly'].apply(lambda x: 1 if x == -1 else 0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Model Evaluation","metadata":{}},{"cell_type":"code","source":"# As we may not have labeled anomalies, we can check the distribution of predicted anomalies\nprint(df['anomaly'].value_counts())\n\n# For supervised evaluation \nfrom sklearn.metrics import classification_report, precision_score\n\n\nprint(classification_report(df['true_label'], df['anomaly']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Precision score\nprint(precision_score(df['true_label'], df['anomaly']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Prediction","metadata":{}},{"cell_type":"code","source":"# Function to predict anomalies on new tickets\ndef predict_anomaly(text):\n    cleaned_text = clean_text(text)\n    embedding = get_bert_embeddings(cleaned_text).reshape(1, -1)\n    prediction = model.predict(embedding)\n    return 1 if prediction == -1 else 0\n\n# Example usage\nnew_ticket = \"This is a new support ticket with some unusual issue.\"\nprint(predict_anomaly(new_ticket))\n","metadata":{},"execution_count":null,"outputs":[]}]}